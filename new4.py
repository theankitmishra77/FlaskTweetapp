# -*- coding: utf-8 -*-
"""applstm6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/154U97DTijEKhrX2RTbtcpKFowLrbgVYS
"""

from flask import Flask,render_template,url_for,request
import pandas as pd 
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from tensorflow.keras.models import Model,load_model
import tensorflow as tf
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))
from nltk.tokenize import word_tokenize
import re
from nltk.stem import WordNetLemmatizer
import re
from nltk.tokenize import word_tokenize
import gensim
import string
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
import pickle

with open('tokenizer.pickle', 'rb') as handle:
		tokenizer = pickle.load(handle)


app = Flask(__name__)

@app.route('/')
def home():
        return render_template('home.html')

def text_Preprocessing(text):
        def remove_emoji(text):
                emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F" 
                           u"\U0001F300-\U0001F5FF" 
                           u"\U0001F680-\U0001F6FF"  
                           u"\U0001F1E0-\U0001F1FF"  
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
                return emoji_pattern.sub(r'', text)
        reviews =[text.lower() for text in text]
        reviews =[text.lower() for text in text]
        reviews =[re.sub(r'\S+@\S+','',text) for text in reviews]
        reviews =[re.sub(r'\d+','',text) for text in reviews]
        reviews =[re.sub(r'[^\w\s]','',text) for text in reviews]
        reviews =[text.strip() for text in reviews]
        reviews =[remove_emoji(text) for text in reviews] 

        stop_words=set(stopwords.words('english'))
  
        cleaned_reviews=[]
        for review in reviews:
                tokens =[word for word in word_tokenize(review) if not word in stop_words]
                cleaned_reviews.append(" ".join(tokens))

        lemmatizer = WordNetLemmatizer()
        lem_reviews=[]
        for review in cleaned_reviews:
                lem_reviews.append(" ".join(list(map(lemmatizer.lemmatize , word_tokenize(review)))))
  
        return lem_reviews

@app.route('/predict',methods=['POST'])
def predict():
        df= pd.read_csv("train.csv")
        from sklearn.model_selection import train_test_split
        train,test=train_test_split(df, test_size=0.3)
        t=train['text']
        train['text'] =text_Preprocessing(t)
        f=test['text']
        test['text'] =text_Preprocessing(f)

        y_train= train['target']
        X_train= train
        X_train.drop('target', axis=1, inplace = True)

        y_test= test['target']
        X_test= test
        X_test.drop('target', axis=1, inplace = True)

        max_review_length = 250

        if request.method == 'POST':
                message = request.form['message']
                d = [message]
                p = text_Preprocessing(d)
                print(p)
                p  = tokenizer.texts_to_sequences(p)
                print(p)
                p_pad = sequence.pad_sequences(p, maxlen=max_review_length , padding='post' )
                print(p_pad)
                model=tf.keras.models.load_model('TweetFinal2.h5')
                my_prediction = model.predict(p_pad)
        return render_template('result.html',prediction = my_prediction[0])



if __name__ == '__main__':
        app.run(debug=True)